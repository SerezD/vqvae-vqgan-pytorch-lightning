image_size: 256

# will encode to 16x16 resolution (256 / 2^4)
autoencoder:
  channels: 64
  num_res_blocks: 2
  channel_multipliers:
    - 1
    - 2
    - 4
    - 8

quantizer:
  num_embeddings: 1024
  embedding_dim: 256
  type: gumbel
  params:  # Taken from DALLE paper: https://arxiv.org/pdf/2102.12092.pdf - appendix A.2
    straight_through: False
    temp: 1.0
    kl_cost: 0.00859375  # 6.6 / [(256 * 256 * 3) / (16 * 16 * 1)]  scale the initial value of 6.6  by the compression factor (768 in our case)
    kl_warmup_epochs: 0.24  # 5000 / 3M total updates.  150 epochs * 0.0016 = 0.24
    temp_decay_epochs: 7.5  # 150000 / 3M total updates. 150 epochs * 0.05 = 7.5
    temp_final: 0.0625 # 1./16.
  reinit_every_n_epochs: 

loss:
  l1_weight: 0.1
  l2_weight: 1.0
  perc_weight: 0.1
  adversarial_params:
    start_epoch: 1  # if None - don't use Discriminator (for ablation purposes)
    loss_type: non-saturating
    g_weight: 0.8  # 0.8 if use adaptive else 0.1 (maskgit)
    use_adaptive: True # compute g weight adaptively according to grad of last layers (then scaled by g_weight)
    r1_reg_weight: 10. # if None don't use r1 regularization (ablation purposes)
    r1_reg_every: 16   # steps

training:
  cumulative_bs: 256
  base_lr: 1e-4  # refers to LR for cumulative_bs = 256. Will scale automatically if bs is increased/reduced
  betas:
    - 0.9
    - 0.999
  eps: 1e-8
  weight_decay: 1e-4
  decay_epochs: 150
  max_epochs: 150
