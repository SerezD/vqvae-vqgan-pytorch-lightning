image_size: 256

# will encode to 16x16 resolution (256 / 2**4)
autoencoder:
  channels: 128
  num_res_blocks: 2
  channel_multipliers:
    - 1
    - 2
    - 2
    - 4

quantizer:
  num_embeddings: 1024  # codebook dimension
  embedding_dim: 256    # single vector dimension
  type: gumbel    # one of ['standard', 'ema', 'gumbel', 'entropy']
  params:  # Taken from DALL-E paper: https://arxiv.org/pdf/2102.12092.pdf - appendix A.2
    straight_through: False
    temp: 1.0
    kl_cost: 0.00859375  # 6.6 / [(256 * 256 * 3) / (16 * 16 * 1)]  scale the initial value of 6.6  by the compression factor (768 in our case)
    kl_warmup_epochs: 0.48  # 5000 / 3M total updates in DALL-E = 0.0016. In this case -> 300 epochs * 0.0016 = 0.48
    temp_decay_epochs: 15  # 150000 / 3M total updates in DALL-E = 0.05. In this case -> 300 epochs * 0.05 = 15
    temp_final: 0.0625 # 1./16.
  reinit_every_n_epochs:     # may be useful in Standard or EMA modes

loss:
  l1_weight: 0.8
  l2_weight: 0.2
  perc_weight: 1.0
  adversarial_params:
    start_epoch: 100  # if None - don't use Discriminator (for ablation purposes)
    loss_type: non-saturating
    g_weight: 0.1  # 0.8 if use adaptive else 0.1 (maskgit)
    use_adaptive: False # compute g weight adaptively according to grad of last layers (then scaled by g_weight)
    r1_reg_weight: 10. # if None don't use r1 regularization (ablation purposes)
    r1_reg_every: 16   # steps

training:
  cumulative_bs: 256
  base_lr: 1e-4  # refers to LR for cumulative_bs = 256. Will scale automatically if bs is increased/reduced
  betas:
    - 0.0
    - 0.99
  eps: 1e-8
  weight_decay: 1e-4
  decay_epochs: 250
  max_epochs: 300
